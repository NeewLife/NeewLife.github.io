---
title: "day 0704"
output:
  html_document:
    keep_md: true
disqusIdentifier: fdsF34ff34
categories: Python
thumbnailImage: image.png
thumbnailImagePosition: left
autoThumbnailImage: yes
metaAlignment: center
date: '2022-07-04 12:50:55'
---

로지스틱 함수

<!-- excerpt -->


## 캐글 데이터
- X_train, X_test, Y_train, Y_test
- X_train, X_val, Y_train, Y-val  같은 함수들을 사용

## 확률적 경사 하강법
- 이 이론이 단독적으로 의미있지는 않지만, 여기서 파생된 이론이 많아서 중요!!
- 점진적 학습 (step, 보폭(=학습속도)
  + 보폭이 클 수록 산에서 빠르게 내려올 수 있지만 여러 경치는 못 본다(좁게 본다)
- 학습률
- XGBoost, LightGBM, 딥러닝(이미지 분류, 자연어 처리, 옵티마이저)

### 손실 함수
- 신경망 이미지 데이터, 자연어
- 자율주행 하루 데이터 1TB --> 이걸 다 학습
- 한꺼번에 전부 모델 학습하기는 어려움
  + 샘플링, 배치, 에포크, 오차(=손실=loss)가 가장 작은 지점을 찾아야 함.
- 결론적으로, 확률적 경사 하강법


#### 로지스틱 손실 함수


```python
import pandas as pd
fish = pd.read_csv("https://bit.ly/fish_csv_data")
fish.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 159 entries, 0 to 158
    Data columns (total 6 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Species   159 non-null    object 
     1   Weight    159 non-null    float64
     2   Length    159 non-null    float64
     3   Diagonal  159 non-null    float64
     4   Height    159 non-null    float64
     5   Width     159 non-null    float64
    dtypes: float64(5), object(1)
    memory usage: 7.6+ KB
    

- 입력 데이터와 타깃 데이터 분리


```python
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
fish_input.shape, fish_target.shape
```




    ((159, 5), (159,))



- 훈련 세트와 테스트 데이터 분리


```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    # input, target, 옵션...
    fish_input, fish_target, random_state=42
)
```

- 훈련 세트와 테스트 세트의 특성 표준화
  + 무게, 길이, 대각선 길이, 높이, 너비
- 단위 표준화 처리 진행


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)


```

### 모델링
- 확률적 경사 하강법


```python
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss = 'log', max_iter = 10, random_state = 42)

sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

# 최적의 장소가 아니라 더 분석해볼 수 있는데 왜 멈췄냐는 메세지가 뜸
```

    0.773109243697479
    0.775
    

    /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      ConvergenceWarning,
    

- partial_fit() 메서드 사용하면 추가 학습.


```python
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.865546218487395
    0.9
    

## 에포크와 과대/과소적합
- 에포크 숫자가 적으면 --> 덜 학습
- early_stopping
  + 에포크 숫자를 1000, 손실 10, 9, 8, ... ,3
  + 3에 도달한 시점이 150
  + 강제로 끊어주는 느낌


```python
import numpy as np
sc = SGDClassifier(loss='log', random_state = 42)
train_score = []
test_score = []

classes = np.unique(train_target)

# 300번 에포크 훈련을 반복
# 훈련 할 때마다, train_score, test_score 추가를 한다.
for _ in range(0, 300):
  sc.partial_fit(train_scaled, train_target, classes = classes)
  train_score.append(sc.score(train_scaled, train_target))
  test_score.append(sc.score(test_scaled, test_target))
```

- 시각화


```python
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.legend(["train", "test"])
plt.show()
# X축 = 에포크, Y축 = 정확도
```


    
![png](/images/0704/output_18_0.png)
    


## 결정 트리(중요!!)
- wine 데이터 가져오기


```python
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine_csv_data')
print(wine.head())
```

       alcohol  sugar    pH  class
    0      9.4    1.9  3.51    0.0
    1      9.8    2.6  3.20    0.0
    2      9.8    2.3  3.26    0.0
    3      9.8    1.9  3.16    0.0
    4      9.4    1.9  3.51    0.0
    

- 데이터 가공하기


```python
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```

- 훈련데이터 분리


```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state = 42
)

train_input.shape, test_input.shape, train_target.shape, test_target.shape
```




    ((5197, 3), (1300, 3), (5197,), (1300,))



- 단위 표준화 처리


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

dt = DecisionTreeClassifier(max_depth = 8, random_state = 42)  # 깊이
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))   # 훈련 세트
print(dt.score(test_scaled, test_target))     # 테스트 세트

plt.figure(figsize = (10, 7))
plot_tree(dt)
plt.show()
```

    0.9003271117952665
    0.8576923076923076
    


    
![png](/images/0704/output_27_1.png)
    


- 훈련 데이터 정확도는 99.7%
- 테스트 데이터 정확도는 85.9%
  + 과대적합이 일어남
  + 정확도가 낮은 이유 : 너무 깊이 세부적으로 들어가기보다 깊이를 1번정도만 했기 때문에
- 정확도가 중요한게 아니고 과대적합이 일어나느냐 마느냐가 중요함
- "max_depth" 의 값을 바꿔가면서 검토
  + max_depth 값을 7로 넣었을 때 보다 8로 넣었을 때 과대적합이 크게 일어난다.
  + 8보다 7이 적합한 수이다.
