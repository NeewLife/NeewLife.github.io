---
title: "day 0701_ch4"
output:
  html_document:
    keep_md: true
disqusIdentifier: fdsF34ff34
categories: Python
thumbnailImage: python.png
thumbnailImagePosition: left
autoThumbnailImage: yes
metaAlignment: center
date: '2022-07-01 17:50:55'
---

### 로지스틱 회귀
<!-- more -->

- 선형 회귀에서 출발
- 이진 분류 문제 해결
- 클래스 확률 예측
- 딥러닝에서도 사용됨

## 교재 177p
- X가 사각형일 확률 30%
- X가 삼각형일 확률  확률 20%50%
- X가 원일

## 데이터 불러오기
- Species (종속변수=Y)
- 나머지 Weight, Length, Diagonal, Height, Width 는 독립변수


```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```





  <div id="df-4733cf17-6355-45a1-8704-1092d897ce3b">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-4733cf17-6355-45a1-8704-1092d897ce3b')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-4733cf17-6355-45a1-8704-1092d897ce3b button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-4733cf17-6355-45a1-8704-1092d897ce3b');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




## 데이터 탐색


```python
# 가장 신경써야 하는건 종속변수
print(pd.unique(fish['Species']))
print(fish['Species'].value_counts())
```

    ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
    Perch        56
    Bream        35
    Roach        20
    Pike         17
    Smelt        14
    Parkki       11
    Whitefish     6
    Name: Species, dtype: int64
    

## 데이터 가공


```python
# pandas 데이터 프레임에서 numpy 배열로 변환
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
print(fish_input.shape)
```

    (159, 5)
    


```python
print(fish_input[:5])
```

    [[242.      25.4     30.      11.52     4.02  ]
     [290.      26.3     31.2     12.48     4.3056]
     [340.      26.5     31.1     12.3778   4.6961]
     [363.      29.      33.5     12.73     4.4555]
     [430.      29.      34.      12.444    5.134 ]]
    

- 타깃 데이터 = 종속변수 = Y


```python
fish_target = fish['Species'].to_numpy()
print(fish_target.shape)
print(fish_target[:5])
```

    (159,)
    ['Bream' 'Bream' 'Bream' 'Bream' 'Bream']
    

## 데이터 분리
- 훈련데이터 테스트 데이터 분리


```python
from sklearn.model_selection import train_test_split

# 임의 샘플링
train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42
)

# 층화 샘플링
```

## 표준화 전처리(중요)
- 여기에서도 훈련 세트의 통계값으로 테스트 세트를 변환해야 한다.
  + 데이터 세트를 나눴을 때 100개의 데이터중 70개는 훈련 세트 데이터, 30개는 테스트 데이터 일 수도 있는데, 테스트를 할때 테스트 데이터를 넣고 돌리면 답지 보고 수능 푸는 격이기 때문에, 우리는 테스트 데이터를 알지만 훈련 데이터 값으로 테스트를 해야 옳은 테스트 이다.
- 예) 데이터 가공 할때
  + 숫자 결측치가 존재, 훈련데이터의 평균값으로 대체 해야 한다.(테스트 데이터의 결측치도 훈련데이터의 평균값으로)
  + 전체데이터의 평균값으로 넣으면 X
  + 따라서 훈련 데이터와 테스트 데이터는 분리 해야한다
- 이 작업을 "데이터 누수(Data Leakage)" 라고 한다
  + 훈련 데이터 평균값 70을 대치
  + 테스트 데이터 평균값 75를 대치
  + 모든 데이터 평균값 73을 대치
  + 이 중 뭐가 옳을까?
    + 답 : 70

교재 97p.
- 기준을 맞춰라 --> 데이터 표준화 (표준점수)
- 수동으로 mean, std
- 표준점수 = (데이터 - 평균) / 분산
 -> ss.transform(tranin_input) 으로 한방에 처리


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
# ss.fit(test_input) => 이 코드는 쓰면 안된다!! 테스트 데이터값은 쓰지 않는다.

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

## 모형 만들기
- K-최근접 이웃


```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors = 3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))
```

    0.8907563025210085
    0.85
    

- 타깃값 확인
- 알파벳 순으로 정렬


```python
print(kn.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    

- 다중분류


```python
print(kn.predict(test_scaled[:5]))
```

    ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
    

- 5개 항목의 확률


```python
import numpy as np
proba = kn.predict_proba(test_scaled[:5])
print (kn.classes_)
print(np.round(proba, decimals=4))
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    [[0.     0.     1.     0.     0.     0.     0.    ]
     [0.     0.     0.     0.     0.     1.     0.    ]
     [0.     0.     0.     1.     0.     0.     0.    ]
     [0.     0.     0.6667 0.     0.3333 0.     0.    ]
     [0.     0.     0.6667 0.     0.3333 0.     0.    ]]
    

- 첫번째 클래스
  + Perch로 분류할 확률이 100%
- 두번째 클래스
  + Perch로 분류할 확률이 66.7%
  + Roach로 분류할 확률이 33.3%

## 회귀 식
- y = ax + b
- 양변에 로그를 취함

## 로지스틱 회귀로 이진 분류 수행하기


```python
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])  # train_scaled 값
# [True, False, True, False, False] --> bream_smelt_indexes
print(char_arr[[True, False, True, False, False]])
```

    ['A' 'C']
    

- True 값만 추출하는 원리를 이용해서
- 이 원리를 이용해서 도미와 빙어의 행만 골라보자


```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

train_scaled.shape, train_bream_smelt.shape
# 119개에서 33개로 줄어든 데이터
```




    ((119, 5), (33, 5))



## 모델 만들기


```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```




    LogisticRegression()




```python
# 클래스를 예측
print(lr.predict(train_bream_smelt[:5]))
```

    ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
    

- 확률 값 구하기


```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

    [[0.99759855 0.00240145]
     [0.02735183 0.97264817]
     [0.99486072 0.00513928]
     [0.98584202 0.01415798]
     [0.99767269 0.00232731]]
    


```python
print(lr.classes_)
# 왼쪽이 Bream 오른쪽이 Smelt
```

    ['Bream' 'Smelt']
    

- 분류기준 : threshold 임계값 설정 (꼭 0.5일 필요는 없다)


도미 vs 빙어

[0.49, 0.51],

[0.70, 0.30],

[0.90, 0.10]

## 계수와 절편


```python
print(lr.coef_, lr.intercept_)
```

    [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
    


```python
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
```

    [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
    

- 이 z 값을 확률 값으로 변환(z값을 평면에 떨어뜨린다)
- 로지스틱 회귀 그래프 원리(직선그래프보다 매끄럽게 이어지는 그래프를 만듬)
![](/images/0701/logic.png)


```python
from scipy.special import expit
print(expit(decisions))
```

    [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
    

## 다중 분류 수행하기
- 이진 분류의 확장판


```python
# 하이퍼 파라미터 세팅
# 모형을 튜닝
# 웬만하면 순정(default) 쓰세요.
# 모형 결과의 과대적합 또는 과소적합을 방지 하기 위한 것
lr = LogisticRegression(C = 20, max_iter = 1000)
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

    0.9327731092436975
    0.925
    


```python
print(lr.predict(test_scaled[:5]))
```

    ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
    


```python
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba,decimals = 3))
print(lr.classes_)
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    

- 다중 분류일 경우 선형 방정식은 어떤 모습일까?
  + 7개 분류, 5개 컬럼 값에 선형 방정식


```python
print(lr.coef_.shape, lr.intercept_.shape)  
```

    (7, 5) (7,)
    

## 평가지표
### 회귀 평가지표
- 결정계수 (교재 121p)
  + 1 - (타깃 - 예측)^2의 합 / (타깃 - 평균)^2의 합
- mae, mse, rmse 만 기억
  + (실제 - 예측) = 오차
  + mae(mean absolute error) : 오차의 절댓값의 평균
  + mse(mean squared error) : 오차 제곱의 평균
  + rmse(root mean squared error) : mse에 제곱근을 취한 값
- 좋은 모델의 기준
  + 결정계수 : 1에 수렴하면 좋은 모델
  + mae, msem, rmse : 0에 수렴하면 좋은 모델


```python
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

true = np.array([1, 2, 3, 2, 3, 5, 4, 6, 5, 6, 7, 8, 8]) # 실제값
preds = np.array([1, 1, 2, 2, 3, 4, 4, 5, 5, 7, 7, 6, 8]) # 예측값

# 절댓값 오차의 평균
mae = mean_absolute_error(true, preds)
print(mae)

# 제곱 오차의 평균
mse = mean_squared_error(true,preds)
print(mse)

# np.squt : 루트 씌워주는 함수
rmse = np.sqrt(mse)
print(rmse)

# 결정계수
r2 = r2_score(true, preds)
print(r2)
```

    0.5384615384615384
    0.6923076923076923
    0.8320502943378437
    0.8617021276595744
    

### 분류 평가지표
- 오차 행렬
- 실제 값
  + [빙어, 도미, 도미, 빙어, 도미]
- 예측 값
  + [빙어, 빙어, 도미, 빙어, 빙어]


TP(빙어를 빙어로 예측) = 2개
TN(도미를 도미로 예측) = 1개
FN(도미를 빙어로 예측) = 2개
FP(빙어를 도미로 예측) = 0개
- TP, TN, FP, FN
- 예) TP = 5, TN = 5, FP = 3, FN = 7
- 모든 케이스에서 정확도로 처리할 수 없으므로 정밀도, 재현율 파악도 중요하다.
  + 정확도 : 전체 중에 맞춘 갯수
    + (TP + TN) / (TP + TN + FP + FN)
  + 정밀도 : 양성(Positive)이라고 예측한 값 중 실제 양성인 값의 비율
    + TP / (TP + FP)
  + 재현율 : 양성(Positive)인 실제 값 중 양성으로 예측한 값(TP)의 비율(암진단 같은 경우)
    + TP / (TP + TF)

  + 로그손실
  + ROC Curve (= AUC)

- 이 모형의 정확도 3/5 = 60%


```python
from sklearn.metrics import confusion_matrix

true = [0,1,1,0,0]
preds = [1,0,0,0,0]

confusion_matrix(true, preds)
```




    array([[2, 1],
           [2, 0]])


