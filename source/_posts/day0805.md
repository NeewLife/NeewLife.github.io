---
title: "딥러닝 실습"
output:
  html_document:
    keep_md: true
disqusIdentifier: fdsF34ff34
categories: Python
clearReading: true
thumbnailImage: python.png
thumbnailImagePosition: left
autoThumbnailImage: yes
metaAlignment: center
date: '2022-08-06 22:08:01'
---


- 딥러닝 실습
<!-- excerpt -->

## 데이터 불러오기


```python
from tensorflow.keras.datasets import imdb
(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)

train_input.shape, test_input.shape
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
    17465344/17464789 [==============================] - 0s 0us/step
    17473536/17464789 [==============================] - 0s 0us/step
    




    ((25000,), (25000,))




```python
print(len(train_input[0]))
print(len(train_input[1]))
```

    218
    189
    


```python
print(train_input[0])
```

    [1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]
    

- 0은 리뷰가 부정적인 문장
- 1은 리뷰가 긍정적인 문장


```python
print(train_target[:20])
```

    [1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]
    

- 훈련데이터 테스트데이터 분리


```python
from sklearn.model_selection import train_test_split
train_input, val_input, train_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42
)

train_input.shape, val_input.shape, train_target.shape, val_target.shape
```




    ((20000,), (5000,), (20000,), (5000,))




```python
import numpy as np
lengths = np.array([len(x) for x in train_input])  # 리스트 컴프리헨션

print(np.mean(lengths), np.median(lengths))
```

    239.00925 178.0
    


```python
import matplotlib.pyplot as plt
plt.hist(lengths)
plt.xlabel('length')
plt.ylabel('frequency')
plt.show()
```


    
![png](/images/0805_1/output_9_0.png)
    


- 20,000개의 리뷰 순회하면서 길이가 100이 되도록 잘라낸다.
- 100개보다 적은 문장은 0으로 채운다.


```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
train_seq = pad_sequences(train_input, maxlen=100)

print(train_seq.shape)
```

    (20000, 100)
    


```python
print(train_seq[0])
```

    [ 10   4  20   9   2 364 352   5  45   6   2   2  33 269   8   2 142   2
       5   2  17  73  17 204   5   2  19  55   2   2  92  66 104  14  20  93
      76   2 151  33   4  58  12 188   2 151  12 215  69 224 142  73 237   6
       2   7   2   2 188   2 103  14  31  10  10 451   7   2   5   2  80  91
       2  30   2  34  14  20 151  50  26 131  49   2  84  46  50  37  80  79
       6   2  46   7  14  20  10  10 470 158]
    


```python
print(train_input[0][-10:])   # 보통 댓글의 결말은 마지막에 작성하는걸로 예상해서 -10으로 거꾸로 추출
```

    [6, 2, 46, 7, 14, 20, 10, 10, 470, 158]
    


```python
print(train_seq[5])
```

    [  0   0   0   0   1   2 195  19  49   2   2 190   4   2 352   2 183  10
      10  13  82  79   4   2  36  71 269   8   2  25  19  49   7   4   2   2
       2   2   2  10  10  48  25  40   2  11   2   2  40   2   2   5   4   2
       2  95  14 238  56 129   2  10  10  21   2  94 364 352   2   2  11 190
      24 484   2   7  94 205 405  10  10  87   2  34  49   2   7   2   2   2
       2   2 290   2  46  48  64  18   4   2]
    


```python
val_seq = pad_sequences(val_input, maxlen=100)
```


```python
train_seq.shape, val_seq.shape
```




    ((20000, 100), (5000, 100))



## 순환신경망 모델 만들기


```python
from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.SimpleRNN(8,input_shape=(100,500)))
model.add(keras.layers.Dense(1, activation='sigmoid'))
```

## LSTM
- Long Short-Term Memory의 약자.
- 단기 기억을 오래 기억하기 위해 고안되었다.


```python
from tensorflow.keras.datasets import imdb
from sklearn.model_selection import train_test_split

(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)

train_input, val_input, train_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42
)

train_input.shape, val_input.shape, train_target.shape, val_target.shape
```




    ((20000,), (5000,), (20000,), (5000,))



- 패딩 작업


```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
train_seq = pad_sequences(train_input, maxlen=100)
val_seq = pad_sequences(val_input, maxlen=100)

train_seq.shape, val_seq.shape
```




    ((20000, 100), (5000, 100))



- LSTM 순환층을 만든다
- 모델 아키텍처를 구성한다


```python
from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.Embedding(500,16,input_length = 100))
model.add(keras.layers.LSTM(8))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding_1 (Embedding)     (None, 100, 16)           8000      
                                                                     
     lstm (LSTM)                 (None, 8)                 800       
                                                                     
     dense_1 (Dense)             (None, 1)                 9         
                                                                     
    =================================================================
    Total params: 8,809
    Trainable params: 8,809
    Non-trainable params: 0
    _________________________________________________________________
    

- 모델 컴파일


```python
rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop, loss = 'binary_crossentropy', metrics = ['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights = True)
history = model.fit(train_seq, train_target, epochs = 100, batch_size = 64,
                    validation_data = (val_seq, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```

    Epoch 1/100
    313/313 [==============================] - 11s 10ms/step - loss: 0.6916 - accuracy: 0.5670 - val_loss: 0.6895 - val_accuracy: 0.6242
    Epoch 2/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.6825 - accuracy: 0.6376 - val_loss: 0.6659 - val_accuracy: 0.6560
    Epoch 3/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.6278 - accuracy: 0.7081 - val_loss: 0.6109 - val_accuracy: 0.7158
    Epoch 4/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.5935 - accuracy: 0.7249 - val_loss: 0.5872 - val_accuracy: 0.7226
    Epoch 5/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.5693 - accuracy: 0.7377 - val_loss: 0.5643 - val_accuracy: 0.7322
    Epoch 6/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.5461 - accuracy: 0.7513 - val_loss: 0.5450 - val_accuracy: 0.7336
    Epoch 7/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.5243 - accuracy: 0.7649 - val_loss: 0.5213 - val_accuracy: 0.7600
    Epoch 8/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.5059 - accuracy: 0.7749 - val_loss: 0.5054 - val_accuracy: 0.7700
    Epoch 9/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4904 - accuracy: 0.7828 - val_loss: 0.4922 - val_accuracy: 0.7832
    Epoch 10/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4774 - accuracy: 0.7923 - val_loss: 0.4813 - val_accuracy: 0.7906
    Epoch 11/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4665 - accuracy: 0.7955 - val_loss: 0.4720 - val_accuracy: 0.7890
    Epoch 12/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4579 - accuracy: 0.8007 - val_loss: 0.4649 - val_accuracy: 0.7916
    Epoch 13/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4504 - accuracy: 0.8019 - val_loss: 0.4591 - val_accuracy: 0.7954
    Epoch 14/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4438 - accuracy: 0.8027 - val_loss: 0.4694 - val_accuracy: 0.7822
    Epoch 15/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4394 - accuracy: 0.8058 - val_loss: 0.4504 - val_accuracy: 0.8002
    Epoch 16/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4349 - accuracy: 0.8077 - val_loss: 0.4487 - val_accuracy: 0.7970
    Epoch 17/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4314 - accuracy: 0.8077 - val_loss: 0.4462 - val_accuracy: 0.7978
    Epoch 18/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4277 - accuracy: 0.8087 - val_loss: 0.4440 - val_accuracy: 0.8014
    Epoch 19/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4257 - accuracy: 0.8094 - val_loss: 0.4465 - val_accuracy: 0.7978
    Epoch 20/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4233 - accuracy: 0.8099 - val_loss: 0.4400 - val_accuracy: 0.8026
    Epoch 21/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4214 - accuracy: 0.8091 - val_loss: 0.4375 - val_accuracy: 0.8034
    Epoch 22/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4199 - accuracy: 0.8110 - val_loss: 0.4361 - val_accuracy: 0.8028
    Epoch 23/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4185 - accuracy: 0.8120 - val_loss: 0.4359 - val_accuracy: 0.8030
    Epoch 24/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4173 - accuracy: 0.8108 - val_loss: 0.4352 - val_accuracy: 0.8036
    Epoch 25/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4164 - accuracy: 0.8107 - val_loss: 0.4353 - val_accuracy: 0.8046
    Epoch 26/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4153 - accuracy: 0.8120 - val_loss: 0.4350 - val_accuracy: 0.7966
    Epoch 27/100
    313/313 [==============================] - 3s 11ms/step - loss: 0.4146 - accuracy: 0.8106 - val_loss: 0.4351 - val_accuracy: 0.8062
    Epoch 28/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4138 - accuracy: 0.8123 - val_loss: 0.4333 - val_accuracy: 0.8014
    Epoch 29/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4132 - accuracy: 0.8116 - val_loss: 0.4336 - val_accuracy: 0.8014
    Epoch 30/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4125 - accuracy: 0.8117 - val_loss: 0.4347 - val_accuracy: 0.7996
    Epoch 31/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4122 - accuracy: 0.8110 - val_loss: 0.4332 - val_accuracy: 0.8046
    Epoch 32/100
    313/313 [==============================] - 3s 10ms/step - loss: 0.4115 - accuracy: 0.8134 - val_loss: 0.4325 - val_accuracy: 0.8010
    Epoch 33/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4114 - accuracy: 0.8122 - val_loss: 0.4356 - val_accuracy: 0.8036
    Epoch 34/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4107 - accuracy: 0.8130 - val_loss: 0.4334 - val_accuracy: 0.8054
    Epoch 35/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4105 - accuracy: 0.8139 - val_loss: 0.4325 - val_accuracy: 0.8038
    

- 모형 학습이 잘 되었는가?


```python
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')  # 오차
plt.legend(['train', 'val'])
plt.show()
# 오차가 점점 많아질수록 과적합 된다는 뜻
```


    
![png](/images/0805_1/output_28_0.png)
    



```python
model = keras.Sequential()
model.add(keras.layers.Embedding(500,16,input_length = 100))
model.add(keras.layers.LSTM(8, dropout=0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))
```


```python
rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop, loss = 'binary_crossentropy', metrics = ['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights = True)
history = model.fit(train_seq, train_target, epochs = 100, batch_size = 64,
                    validation_data = (val_seq, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])


```

    Epoch 1/100
    313/313 [==============================] - 5s 10ms/step - loss: 0.4186 - accuracy: 0.8083 - val_loss: 0.4319 - val_accuracy: 0.8032
    Epoch 2/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4169 - accuracy: 0.8098 - val_loss: 0.4337 - val_accuracy: 0.7972
    Epoch 3/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4154 - accuracy: 0.8113 - val_loss: 0.4326 - val_accuracy: 0.8012
    Epoch 4/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4161 - accuracy: 0.8099 - val_loss: 0.4310 - val_accuracy: 0.8018
    Epoch 5/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4144 - accuracy: 0.8134 - val_loss: 0.4312 - val_accuracy: 0.8032
    Epoch 6/100
    313/313 [==============================] - 3s 11ms/step - loss: 0.4143 - accuracy: 0.8106 - val_loss: 0.4307 - val_accuracy: 0.7992
    Epoch 7/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4139 - accuracy: 0.8127 - val_loss: 0.4295 - val_accuracy: 0.8018
    Epoch 8/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4133 - accuracy: 0.8118 - val_loss: 0.4294 - val_accuracy: 0.8028
    Epoch 9/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4125 - accuracy: 0.8112 - val_loss: 0.4291 - val_accuracy: 0.8034
    Epoch 10/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4113 - accuracy: 0.8129 - val_loss: 0.4290 - val_accuracy: 0.8034
    Epoch 11/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4106 - accuracy: 0.8111 - val_loss: 0.4314 - val_accuracy: 0.8046
    Epoch 12/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4095 - accuracy: 0.8145 - val_loss: 0.4279 - val_accuracy: 0.8018
    Epoch 13/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4099 - accuracy: 0.8141 - val_loss: 0.4275 - val_accuracy: 0.8032
    Epoch 14/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4098 - accuracy: 0.8128 - val_loss: 0.4299 - val_accuracy: 0.8036
    Epoch 15/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4087 - accuracy: 0.8153 - val_loss: 0.4282 - val_accuracy: 0.8052
    Epoch 16/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4081 - accuracy: 0.8141 - val_loss: 0.4272 - val_accuracy: 0.8064
    Epoch 17/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4074 - accuracy: 0.8158 - val_loss: 0.4285 - val_accuracy: 0.8016
    Epoch 18/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4065 - accuracy: 0.8140 - val_loss: 0.4286 - val_accuracy: 0.8006
    Epoch 19/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4075 - accuracy: 0.8152 - val_loss: 0.4255 - val_accuracy: 0.8036
    Epoch 20/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4080 - accuracy: 0.8117 - val_loss: 0.4265 - val_accuracy: 0.8026
    Epoch 21/100
    313/313 [==============================] - 3s 8ms/step - loss: 0.4060 - accuracy: 0.8152 - val_loss: 0.4252 - val_accuracy: 0.8034
    Epoch 22/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4065 - accuracy: 0.8149 - val_loss: 0.4258 - val_accuracy: 0.8040
    Epoch 23/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4051 - accuracy: 0.8164 - val_loss: 0.4251 - val_accuracy: 0.8042
    Epoch 24/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4059 - accuracy: 0.8167 - val_loss: 0.4249 - val_accuracy: 0.8036
    Epoch 25/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4037 - accuracy: 0.8162 - val_loss: 0.4244 - val_accuracy: 0.8034
    Epoch 26/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4041 - accuracy: 0.8160 - val_loss: 0.4262 - val_accuracy: 0.8062
    Epoch 27/100
    313/313 [==============================] - 3s 10ms/step - loss: 0.4021 - accuracy: 0.8152 - val_loss: 0.4245 - val_accuracy: 0.8050
    Epoch 28/100
    313/313 [==============================] - 3s 9ms/step - loss: 0.4020 - accuracy: 0.8162 - val_loss: 0.4270 - val_accuracy: 0.8056
    




    [<matplotlib.lines.Line2D at 0x7f9996fbb050>]




    
![png](/images/0805_1/output_30_2.png)
    



```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')  # 오차
plt.legend(['train', 'val'])
plt.show()
```


    
![png](/images/0805_1/output_31_0.png)
    


- 2개의 층을 연결하기


```python
model = keras.Sequential()
model.add(keras.layers.Embedding(500,16,input_length = 100))
model.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True))
model.add(keras.layers.LSTM(8, dropout=0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))

rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop, loss = 'binary_crossentropy', metrics = ['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights = True)
history = model.fit(train_seq, train_target, epochs = 100, batch_size = 64,
                    validation_data = (val_seq, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```
